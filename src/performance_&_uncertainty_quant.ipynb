{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791feaec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1714550399698,
     "user": {
      "displayName": "Thomas Bonnier",
      "userId": "10664136810129346668"
     },
     "user_tz": -120
    },
    "id": "791feaec",
    "outputId": "d02ebb70-4cd3-45a9-d5fa-24c2756d3bff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c30a4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5868,
     "status": "ok",
     "timestamp": 1714550406361,
     "user": {
      "displayName": "Thomas Bonnier",
      "userId": "10664136810129346668"
     },
     "user_tz": -120
    },
    "id": "55c30a4d",
    "outputId": "35303dde-3c10-473d-d23b-9c41b7e3016c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Pytorch version: 2.2.1+cu121\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c873e7",
   "metadata": {
    "id": "69c873e7"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, DistilBertTokenizerFast, DistilBertModel\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import time\n",
    "\n",
    "# useful .py\n",
    "from settings import * # settings\n",
    "from dataset import * # data pre-processing\n",
    "from model import * # models\n",
    "from optimization import * # model selection, training, evaluation\n",
    "from uncertainty import * # uncertainty quantification and xai\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d59c86",
   "metadata": {
    "id": "22d59c86"
   },
   "source": [
    "### Experiments for TTT, TTT-SRP, non pretrained baselines (EarlyConcat, LateFuse, MulT, TFN), and ablation studies\n",
    "\n",
    "\n",
    "*   performance\n",
    "*   uncertainty quantification (SD, LAC, MCD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388da219",
   "metadata": {
    "id": "388da219"
   },
   "outputs": [],
   "source": [
    "# load settings\n",
    "DATASET = \"cloth\" # choose in {\"airbnb\", \"cloth\", \"jigsaw\", \"kick\", \"pet\", \"salary, \"wine_10\", \"wine_100\"}\n",
    "FILENAME, categorical_var, numerical_var, text_var, MAX_LEN_QUANTILE, N_CLASSES, WEIGHT_DECAY, FACTOR, N_EPOCHS, split_val, CRITERION, N_SEED, DROPOUT= load_settings(dataset = DATASET)\n",
    "\n",
    "perf_results = pd.DataFrame()\n",
    "uncertainty_results = pd.DataFrame()\n",
    "val_uncertainty_results = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for SEED in range(N_SEED):\n",
    "\n",
    "    for MODEL_TYPE in [\"MulT\", \"EarlyConcat\", \"LateFuse\", \"TFN\", \"TTT\", \"TTT-SRP\", \"TTT_ablation1\", \"TTT_ablation2\", \"TTT_ablation3\", \"TTT-PCA\", \"TTT-Kaiming\"]:\n",
    "\n",
    "        start = time.time()\n",
    "        perf_results.loc[i,\"seed\"] = SEED\n",
    "        perf_results.loc[i,\"model type\"] = MODEL_TYPE\n",
    "\n",
    "        # GPU or CPU\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # load and prepare dataset\n",
    "        df = preprocess_dataset(DATASET, MODEL_TYPE)\n",
    "\n",
    "        # Train/Test split\n",
    "        df, target = train_test_split(df, test_size = split_val, random_state = SEED)\n",
    "\n",
    "        if MODEL_TYPE in [\"TTT-SRP\", \"TTT-PCA\", \"TTT-Kaiming\"]:\n",
    "            PATIENCE = 1\n",
    "            # text cleaning (keep only words and numbers)\n",
    "            df['clean_text'] = df[text_var].apply(lambda row:clean_text(row))\n",
    "            target['clean_text'] = target[text_var].apply(lambda row:clean_text(row))\n",
    "\n",
    "            # Load the specific tokenizer and text model\n",
    "            tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "            text_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "            # text max length\n",
    "            MAX_LEN = int(np.quantile(df.apply(lambda row : len(tokenizer(row['clean_text']).input_ids), axis=1).values, q = [MAX_LEN_QUANTILE]).item())\n",
    "            MAX_LEN = min(MAX_LEN, 512) # maximum sequence length is 512 for DistilBERT\n",
    "\n",
    "            # vocabulary construction\n",
    "            VOCAB_SIZE = text_model.embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "        else:\n",
    "            PATIENCE = 4\n",
    "            # text max length\n",
    "            MAX_LEN = int(np.quantile(df.apply(lambda row : len(row[text_var].split()), axis=1).values, q = [MAX_LEN_QUANTILE]).item())\n",
    "\n",
    "            # vocabulary construction\n",
    "            vocab2index, VOCAB_SIZE, words = vocabulary(df, text_field = text_var)\n",
    "\n",
    "            # encode text on Source and Target\n",
    "            df['encoded_var'] = df[text_var].apply(lambda x: encode_sentence(x,vocab2index,max_len=MAX_LEN))\n",
    "            target['encoded_var'] = target[text_var].apply(lambda x: encode_sentence(x,vocab2index,max_len=MAX_LEN))\n",
    "\n",
    "        perf_results.loc[i,\"max text length\"] = MAX_LEN\n",
    "        perf_results.loc[i,\"vocab size\"] = VOCAB_SIZE\n",
    "\n",
    "        # Numerical variables pre-processing\n",
    "        numerical_var_scaled = standardScaling(df, target, numerical_var)\n",
    "        QUANTILES = []\n",
    "        for var in numerical_var_scaled:\n",
    "            QUANTILES.append(np.quantile(df[var].values, q = [0., 0.2, 0.4, 0.6, 0.8, 1.]))\n",
    "        NUM_NUMERICAL_VAR = len(numerical_var)\n",
    "\n",
    "        # Categorical variables pre-processing\n",
    "        if MODEL_TYPE in [\"LateFuse\", \"TFN\"]:\n",
    "            categorical_var_oe = oneHotEncoding(df, target, categorical_var)\n",
    "            NUM_CAT_VAR = len(categorical_var_oe)\n",
    "            CAT_VOCAB_SIZES = None\n",
    "        else:\n",
    "            categorical_var_oe, CAT_VOCAB_SIZES = ordinalEncoding(df, target, categorical_var)\n",
    "            NUM_CAT_VAR = len(categorical_var)\n",
    "\n",
    "        # train / validation split\n",
    "        df_train, df_validation = train_test_split(df, test_size = split_val, random_state = SEED)\n",
    "        perf_results.loc[i,\"training size\"] = df_train.shape[0]\n",
    "        perf_results.loc[i,\"test size\"] = target.shape[0]\n",
    "\n",
    "        if MODEL_TYPE in [\"TTT-SRP\", \"TTT-PCA\", \"TTT-Kaiming\"]:\n",
    "            # prepare the Tensor Datasets, including tokenization\n",
    "            dataset_train = prepareTensorDatasetWithTokenizer(df_train, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=False, model_type = MODEL_TYPE)\n",
    "            dataset_validation = prepareTensorDatasetWithTokenizer(df_validation, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=False, model_type = MODEL_TYPE)\n",
    "            dataset_target = prepareTensorDatasetWithTokenizer(target, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=False, model_type = MODEL_TYPE)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # prepare custom datasets\n",
    "            dataset_train, dataset_validation, dataset_target = prepareCustomDatasets(df_train, df_validation, target,\n",
    "                                                                                  encoded_text_var = 'encoded_var',\n",
    "                                                                                  categorical_var = categorical_var_oe,\n",
    "                                                                                  numerical_var = numerical_var_scaled,\n",
    "                                                                                  label = 'Y')\n",
    "\n",
    "        # HPO\n",
    "        if MODEL_TYPE in [\"TTT-SRP\", \"TTT-PCA\", \"TTT-Kaiming\"]:\n",
    "            if MODEL_TYPE == \"TTT-SRP\":\n",
    "              init_weights = \"random_proj\"\n",
    "            elif MODEL_TYPE == \"TTT-PCA\":\n",
    "              init_weights = \"pca\"\n",
    "            else:\n",
    "              init_weights = \"kaiming\"\n",
    "            best_params = hp_optimization_large(MODEL_TYPE,dataset_train, dataset_validation,\n",
    "                                              text_model, init_weights,\n",
    "                                              MAX_LEN, VOCAB_SIZE, CAT_VOCAB_SIZES, NUM_CAT_VAR, NUM_NUMERICAL_VAR, QUANTILES,\n",
    "                                              criterion=CRITERION, seed=SEED, device = device, dataset = DATASET)\n",
    "\n",
    "        else:\n",
    "            best_params = hp_optimization(MODEL_TYPE,dataset_train, dataset_validation,\n",
    "                                          MAX_LEN, VOCAB_SIZE, CAT_VOCAB_SIZES, NUM_CAT_VAR, NUM_NUMERICAL_VAR, QUANTILES,\n",
    "                                          criterion=CRITERION, seed=SEED, device = device, dataset = DATASET)\n",
    "\n",
    "\n",
    "        LR, BATCH_SIZE, D_MODEL, N_LAYERS, N_HEADS = best_params['LR'],best_params['BATCH_SIZE'],best_params['D_MODEL'],best_params['N_LAYERS'],best_params['N_HEADS']\n",
    "\n",
    "        # same dimension for Feed Forward and Fully Connected\n",
    "        D_FF = D_MODEL\n",
    "        D_FC = D_MODEL\n",
    "\n",
    "        perf_results.loc[i,\"LR\"] = LR\n",
    "        perf_results.loc[i,\"BATCH_SIZE\"] = BATCH_SIZE\n",
    "        perf_results.loc[i,\"D_MODEL\"] = D_MODEL\n",
    "        perf_results.loc[i,\"N_LAYERS\"] = N_LAYERS\n",
    "        perf_results.loc[i,\"N_HEADS\"] = N_HEADS\n",
    "\n",
    "        # data loaders\n",
    "        if MODEL_TYPE in [\"TTT-SRP\", \"TTT-PCA\", \"TTT-Kaiming\"]:\n",
    "            loader_train = DataLoader(dataset_train, sampler = RandomSampler(dataset_train), batch_size = BATCH_SIZE)\n",
    "            loader_validation = DataLoader(dataset_validation, sampler = SequentialSampler(dataset_validation),batch_size = BATCH_SIZE)\n",
    "            loader_target = DataLoader(dataset_target, sampler = SequentialSampler(dataset_target),batch_size = BATCH_SIZE)\n",
    "        else:\n",
    "            loader_train = DataLoader(dataset_train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "            loader_validation = DataLoader(dataset_validation, batch_size = BATCH_SIZE, shuffle = True)\n",
    "            loader_target = DataLoader(dataset_target, batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "        # model initialization\n",
    "        model = init_model(model_type = MODEL_TYPE,\n",
    "                           d_model = D_MODEL,\n",
    "                           max_len = MAX_LEN,\n",
    "                           vocab_size = VOCAB_SIZE,\n",
    "                           cat_vocab_sizes = CAT_VOCAB_SIZES,\n",
    "                           num_cat_var = NUM_CAT_VAR,\n",
    "                           num_numerical_var = NUM_NUMERICAL_VAR,\n",
    "                           quantiles = QUANTILES,\n",
    "                           n_heads = N_HEADS,\n",
    "                           d_ff = D_FF,\n",
    "                           n_layers = N_LAYERS,\n",
    "                           dropout = DROPOUT,\n",
    "                           d_fc = D_FC,\n",
    "                           n_classes = N_CLASSES,\n",
    "                           seed = SEED,\n",
    "                           device=device).to(device)\n",
    "\n",
    "        # specific initialization for input embeddings\n",
    "        if MODEL_TYPE in [\"TTT-SRP\", \"TTT-PCA\"]:\n",
    "            # Load embeddings weights from pretrained model\n",
    "            pretrained_dict = text_model.state_dict()\n",
    "            model_dict = model.state_dict()\n",
    "            updated_model_dict = {k: v for k, v in model_dict.items()}\n",
    "            distil_bert_embeddings = text_model.state_dict()['embeddings.word_embeddings.weight']\n",
    "\n",
    "            # dimension reduction\n",
    "            if MODEL_TYPE == \"TTT-PCA\":\n",
    "                reduction_technique = PCA(n_components=D_MODEL, random_state = SEED)\n",
    "            if MODEL_TYPE == \"TTT-SRP\":\n",
    "                reduction_technique = SparseRandomProjection(n_components=D_MODEL, random_state = SEED)\n",
    "            reduced_embeddings = torch.tensor(reduction_technique.fit_transform(distil_bert_embeddings.cpu().numpy())).to(device)\n",
    "\n",
    "            # update embeddings\n",
    "            updated_model_dict.update([('text_embedding.weight', reduced_embeddings)])\n",
    "            model_dict.update(updated_model_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "        # number of trainable parameters\n",
    "        perf_results.loc[i,\"trainable parameters\"] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # training\n",
    "        model = training(model, loader_train,  N_EPOCHS, loader_validation, CRITERION, optimizer, PATIENCE, FACTOR, MODEL_TYPE, SEED, verbose=False, device = device, early_stopping = True)\n",
    "\n",
    "        # model evaluation and uncertainty quantification\n",
    "        model.eval()\n",
    "        if MODEL_TYPE in [\"MulT\", \"EarlyConcat\", \"LateFuse\", \"TFN\"]:\n",
    "            perf = performance(model, loader_target, MODEL_TYPE, SEED, device)\n",
    "            perf_results.loc[i,\"performance\"] = perf\n",
    "            perf_results.loc[i,\"coverage\"] = \"no\"\n",
    "            perf_results.loc[i,\"interval_width\"] = \"no\"\n",
    "            perf_results.loc[i,\"validation coverage\"] = \"no\"\n",
    "        else:\n",
    "            # target: performance\n",
    "            perf, labels, preds, text_preds, tabular_preds = performance(model, loader_target, MODEL_TYPE, SEED, device)\n",
    "            perf_results.loc[i,\"performance\"] = perf\n",
    "            uncertainty_results[\"labels-\"+str(SEED)] = labels\n",
    "            uncertainty_results[\"preds-\"+str(SEED)] = preds\n",
    "            uncertainty_results[\"text_preds-\"+str(SEED)] = text_preds\n",
    "            uncertainty_results[\"tabular_preds-\"+str(SEED)] = tabular_preds\n",
    "\n",
    "            # (dis)agreement accuracy\n",
    "            disagr_df = uncertainty_results[uncertainty_results[\"text_preds-\"+str(SEED)]!=uncertainty_results[\"tabular_preds-\"+str(SEED)]]\n",
    "            agr_df = uncertainty_results[uncertainty_results[\"text_preds-\"+str(SEED)]==uncertainty_results[\"tabular_preds-\"+str(SEED)]]\n",
    "            disagr_acc = (disagr_df[\"labels-\"+str(SEED)]==disagr_df[\"preds-\"+str(SEED)]).sum()/len(disagr_df)\n",
    "            agr_acc = (agr_df[\"labels-\"+str(SEED)]==agr_df[\"preds-\"+str(SEED)]).sum()/len(agr_df)\n",
    "            perf_results.loc[i,\"disagreement accuracy\"] = disagr_acc\n",
    "            perf_results.loc[i,\"agreement accuracy\"] = agr_acc\n",
    "\n",
    "            # validation: performance\n",
    "            _, val_labels, val_preds, val_text_preds, val_tabular_preds = performance(model, loader_validation, MODEL_TYPE, SEED, device)\n",
    "            val_uncertainty_results[\"labels-\"+str(SEED)] = val_labels\n",
    "            val_uncertainty_results[\"preds-\"+str(SEED)] = val_preds\n",
    "            val_uncertainty_results[\"text_preds-\"+str(SEED)] = val_text_preds\n",
    "            val_uncertainty_results[\"tabular_preds-\"+str(SEED)] = val_tabular_preds\n",
    "\n",
    "            # uncertainty quantification: TTT coverage and prediction set size\n",
    "            diff = uncertainty_results[ \"text_preds-\"+str(SEED)] != uncertainty_results[ \"tabular_preds-\"+str(SEED)]\n",
    "            interval_width = np.mean(diff+1)\n",
    "            cov = ((uncertainty_results[\"labels-\"+str(SEED)] == uncertainty_results[\"text_preds-\"+str(SEED)]) | (uncertainty_results[\"labels-\"+str(SEED)] == uncertainty_results[ \"tabular_preds-\"+str(SEED)]))\n",
    "            coverage = np.mean(cov)\n",
    "            val_cov = ((val_uncertainty_results[\"labels-\"+str(SEED)] == val_uncertainty_results[\"text_preds-\"+str(SEED)]) | (val_uncertainty_results[\"labels-\"+str(SEED)] == val_uncertainty_results[ \"tabular_preds-\"+str(SEED)]))\n",
    "            val_coverage = np.mean(val_cov)\n",
    "            perf_results.loc[i,\"validation coverage\"] = val_coverage\n",
    "            perf_results.loc[i,\"coverage\"] = coverage\n",
    "            perf_results.loc[i,\"interval_width\"] = interval_width\n",
    "\n",
    "        if MODEL_TYPE in [\"TTT\"]:\n",
    "            # uncertainty quantification: conformal prediction (LAC baseline for uncertainty quantification)\n",
    "            cp_coverage, cp_interval_width = conformal_prediction(loader_validation,\n",
    "                                                        loader_target,\n",
    "                                                        model,\n",
    "                                                        model_type = MODEL_TYPE,\n",
    "                                                        target_coverage = val_coverage,\n",
    "                                                        seed = SEED,\n",
    "                                                        device = device)\n",
    "            perf_results.loc[i,\"CP coverage\"] = cp_coverage\n",
    "            perf_results.loc[i,\"CP interval_width\"] = cp_interval_width\n",
    "\n",
    "            # compute Shannon entropy with Bayesian MCD technique\n",
    "            total_entropy, aleatoric_entropy = compute_MCD(model, loader_target, n_simu = 50, seed = SEED, device = device)\n",
    "            perf_results.loc[i,\"total_entropy\"] = total_entropy\n",
    "            perf_results.loc[i,\"aleatoric_entropy\"] = aleatoric_entropy\n",
    "\n",
    "        else:\n",
    "            perf_results.loc[i,\"CP coverage\"] = \"no\"\n",
    "            perf_results.loc[i,\"CP interval_width\"] = \"no\"\n",
    "            perf_results.loc[i,\"total_entropy\"] = \"no\"\n",
    "            perf_results.loc[i,\"aleatoric_entropy\"] = \"no\"\n",
    "\n",
    "        elapsed_time = time.time()-start\n",
    "        perf_results.loc[i,\"time\"] = elapsed_time\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    display(perf_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_1ok5M2xugi",
   "metadata": {
    "id": "e_1ok5M2xugi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rLK1Q_Dsxurp",
   "metadata": {
    "id": "rLK1Q_Dsxurp"
   },
   "source": [
    "### Experiments for pretrained models (AllTextBERT and LateFuseBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EyiDjRYuRarP",
   "metadata": {
    "id": "EyiDjRYuRarP"
   },
   "outputs": [],
   "source": [
    "# select DATASET\n",
    "DATASET = \"cloth\" # choose in {\"airbnb\", \"cloth\", \"jigsaw\", \"kick\", \"pet\", \"salary, \"wine_10\", \"wine_100\"}\n",
    "FILENAME, categorical_var, numerical_var, text_var, MAX_LEN_QUANTILE, N_CLASSES, WEIGHT_DECAY, FACTOR, N_EPOCHS, split_val, CRITERION, N_SEED, DROPOUT= load_settings(dataset = DATASET)\n",
    "\n",
    "# performance records\n",
    "perf_results = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for SEED in range(N_SEED):\n",
    "\n",
    "    for MODEL_TYPE in [\"LateFuseBERT\", \"AllTextBERT\"]:\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # GPU or CPU\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # load and prepare dataset\n",
    "            df = preprocess_dataset(DATASET, MODEL_TYPE)\n",
    "\n",
    "            # control randomness\n",
    "            random.seed(SEED)\n",
    "            np.random.seed(SEED)\n",
    "            torch.manual_seed(SEED)\n",
    "            torch.cuda.manual_seed(SEED)\n",
    "\n",
    "            # temporary dataframes to compute uncertainty metrics\n",
    "            uncertainty_results = pd.DataFrame()\n",
    "            val_uncertainty_results = pd.DataFrame()\n",
    "\n",
    "            perf_results.loc[i,\"model type\"] = MODEL_TYPE\n",
    "            perf_results.loc[i,\"seed\"] = SEED\n",
    "\n",
    "            # Train/Test split\n",
    "            df, target = train_test_split(df, test_size = split_val, random_state = SEED)\n",
    "\n",
    "            # text cleaning (keep only words and numbers)\n",
    "            df['clean_text'] = df[text_var].apply(lambda row:clean_text(row))\n",
    "            target['clean_text'] = target[text_var].apply(lambda row:clean_text(row))\n",
    "\n",
    "            # Load the specific tokenizer\n",
    "            tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "            # text max length\n",
    "            MAX_LEN = int(np.quantile(df.apply(lambda row : len(tokenizer(row['clean_text']).input_ids), axis=1).values, q = [MAX_LEN_QUANTILE]).item())\n",
    "            MAX_LEN = min(MAX_LEN, 512) # maximum sequence length is 512 for DistilBERT\n",
    "            perf_results.loc[i,\"max text length\"] = MAX_LEN\n",
    "\n",
    "            # Numerical variables pre-processing\n",
    "            numerical_var_scaled = standardScaling(df, target, numerical_var)\n",
    "            NUM_NUMERICAL_VAR = len(numerical_var)\n",
    "            QUANTILES = []\n",
    "            for var in numerical_var_scaled:\n",
    "                QUANTILES.append(np.quantile(df[var].values, q = [0., 0.2, 0.4, 0.6, 0.8, 1.]))\n",
    "\n",
    "            # Categorical variables pre-processing\n",
    "            categorical_var_oe, CAT_VOCAB_SIZES = ordinalEncoding(df, target, categorical_var)\n",
    "            NUM_CAT_VAR = len(categorical_var)\n",
    "\n",
    "            # train / validation split\n",
    "            df_train, df_validation = train_test_split(df, test_size = split_val, random_state = SEED)\n",
    "            perf_results.loc[i,\"training size\"] = df_train.shape[0]\n",
    "            perf_results.loc[i,\"test size\"] = target.shape[0]\n",
    "\n",
    "            # hyper-parameters\n",
    "            LR, BATCH_SIZE, D_FC, N_EPOCHS, N_HEADS, N_LAYERS = load_pretrained_settings()\n",
    "            perf_results.loc[i,\"LR\"] = LR\n",
    "            perf_results.loc[i,\"BATCH_SIZE\"] = BATCH_SIZE\n",
    "            perf_results.loc[i,\"N_HEADS\"] = N_HEADS\n",
    "            perf_results.loc[i,\"N_LAYERS\"] = N_LAYERS\n",
    "\n",
    "            # prepare the Tensor Datasets, including tokenization\n",
    "            dataset_train = prepareTensorDatasetWithTokenizer(df_train, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=True, model_type = MODEL_TYPE)\n",
    "            dataset_validation = prepareTensorDatasetWithTokenizer(df_validation, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=True, model_type = MODEL_TYPE)\n",
    "            dataset_target = prepareTensorDatasetWithTokenizer(target, \"clean_text\", categorical_var_oe, numerical_var_scaled, 'Y', tokenizer, MAX_LEN, special_tokens=True, model_type = MODEL_TYPE)\n",
    "\n",
    "            # data loaders\n",
    "            loader_train = DataLoader(dataset_train, sampler = RandomSampler(dataset_train), batch_size = BATCH_SIZE)\n",
    "            loader_validation = DataLoader(dataset_validation, sampler = SequentialSampler(dataset_validation),batch_size = BATCH_SIZE)\n",
    "            loader_target = DataLoader(dataset_target, sampler = SequentialSampler(dataset_target),batch_size = BATCH_SIZE)\n",
    "\n",
    "            # Load Bert with a linear classification layer\n",
    "            BERT_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "            # model initialization\n",
    "            torch.manual_seed(SEED)\n",
    "            model = init_model(model_type = MODEL_TYPE,\n",
    "                               d_model = BERT_model.embeddings.word_embeddings.embedding_dim, # dimension = 768 for BERT family\n",
    "                               max_len = \"\", # not used here\n",
    "                               vocab_size = \"\", # not used here\n",
    "                               cat_vocab_sizes = CAT_VOCAB_SIZES,\n",
    "                               num_cat_var = NUM_CAT_VAR,\n",
    "                               num_numerical_var = NUM_NUMERICAL_VAR,\n",
    "                               quantiles = \"\", # not used here\n",
    "                               n_heads = N_HEADS,\n",
    "                               d_ff = \"\", # not used here\n",
    "                               n_layers = N_LAYERS,\n",
    "                               dropout = DROPOUT,\n",
    "                               d_fc = D_FC,\n",
    "                               n_classes = N_CLASSES,\n",
    "                               seed = SEED,\n",
    "                               device=device,\n",
    "                               text_model = BERT_model).to(device)\n",
    "\n",
    "            # number of trainable parameters\n",
    "            perf_results.loc[i,\"trainable parameters\"] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            # optimizer\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "            # training\n",
    "            model, epochs = training_pretrained(model, MODEL_TYPE, loader_train,  N_EPOCHS, loader_validation, CRITERION, optimizer, FACTOR, SEED, verbose=True, device = device)\n",
    "            perf_results.loc[i,\"epochs\"] = epochs\n",
    "\n",
    "            # model evaluation\n",
    "            model.eval()\n",
    "\n",
    "            target_perf = performance_pretrained(model, loader_target, MODEL_TYPE, SEED, device)\n",
    "            perf_results.loc[i,\"performance (Target)\"] = target_perf\n",
    "\n",
    "            elapsed_time = time.time()-start\n",
    "            perf_results.loc[i,\"time\"] = elapsed_time\n",
    "\n",
    "            i+=1\n",
    "\n",
    "    display(perf_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vquoozMZ2Hv7",
   "metadata": {
    "id": "vquoozMZ2Hv7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
